\chapter{Literature Review}\label{chap:2}

\section{Overview}

This section provides a broader context of this area concerning other researches and studies. The literature review consists of the following areas. 

\begin{enumerate}
	\item Characteristics of IO 
	\item Performance of web server architectures
	\item Importance of thread pool tuning
	\item Appropriateness of Ballerina for this research
	
\end{enumerate}

\begin{comment}
	Following few sections span some core topics related to the research and effort of other researches who put in this area.
	
	A web server needs to support millions of users demanding access to services that must be responsive,
	robust, and always available. The number of concurrent sessions and hits per day to web servers
	translates into an even higher number of I/O and network requests, placing enormous demands on
	underlying resources. 
	
	This research started with boarder area of web server architecture optimizations. The aim was to identify optimal web server architecture in Ballerina for given type of workload or program. Exploration mainly started with identifying IO features of the programs when classifying the workload. Then this research able to identify prominent IO features in Ballerina using extensive testing. Based on those IO features this research able to propose optimal thread pool size using machine learning algorithms. Finally narrowed down to finding the optimal number of threads in thread pool for given program.
\end{comment}




\section{Background and related theories}

\subsection{About IO}

Basically, every program instruction can be divided into IO and non-IO operations. IO operations are
typically operations that do not use the CPU. When there is an API call that requests data from IO,
that call won’t immediately return, but with some delay. \cite{nb_algo} This delay can be very small for
requesting a file on a hard drive, and much longer when requesting data from a network. This is
because the data that is requested from IO devices has to travel longer to the caller. For example, a call to an external HTTP server has to wait more rather than requesting data from a server where hosted on the same machine.
There are two types of IO,

\begin{enumerate}
	\item Blocking IO
	\item Non-blocking IO
\end{enumerate}

Calling an operation that requests data from IO will cause the running thread to "block", i.e. it is
waiting until the requested data has returned to the caller. When a thread is blocked in Linux, it will be put in a Sleep state by the kernel
until data has returned to the caller. Threads in sleep state immediately give up their access to the CPU,
so as to not waste CPU time. After IO is ready, the thread is taken out from the Sleep state and put in a
Runnable state. Threads in this state are eligible to be executed on the CPU again. The thread
scheduler will put the thread on a CPU when one is available. The process of taking threads on and off
the CPU is called context switching where this process is done at OS level. Context switching is quite expensive because it has to save the data related to its state called context and when it is ready to run, data must be restored again into CPU registers and stacks. The allocated memory size of thread in java is 1 Mb by default. A web server receiving 1000 concurrent requests may result in 1000Mb usage of RAM only to maintain the thread. Moreover, when there is a large number of context switching is happening larger overhead is added to the system. This is where the disadvantage of the thread per connection model is apparent.

Non-blocking IO reduces the context switching by reusing the threads. The main benefit of non-blocking IO
is that we need fewer threads to handle the same amount of IO requests. When there is an IO operation that needs to be performed, the task is delegated to OS and so that call is kept in a queue, then thread is released for other operations. When IO is completed, the queue is notified and further instructions of the task are carried out afterward. Non-blocking IO is tightly coupled with thread pools. The fewer threads are allocated in advance and tasks are executed by those threads by reusing. Therefore, context switching is minimized.

\subsection{Event loops}
To facilitate the non-blocking IO most programming languages implement an event loop. The concept
of the event can be any operation including IO operations needed to be executed. The event loop polls
(constantly check) if data is returned from IO. An event loop is a while(true) loop that in
each iteration will check if data is ready to read from a File Descriptor in Unix systems \cite{device_file}. The
list of File descriptors that we want to check for ready data is called the interest list. At glance, we can
think of the event loop as an expensive task. Although several optimization techniques have been implemented at the kernel level to reduce the cost of the event loop. Some examples are epoll,
io{\_}uring, kqueue \cite{web_pipeline,io_uring}.


\subsection{Web Server architecture performance}

There are several web server architectures which have existed for a long time. In the early 90s, the thread per connection model was popular because it is relatively easier to implement. But when the internet is getting popular, web servers get more hits. The thread per connection model has serious flaws when there are higher numbers of concurrency reaches. Since each thread needs a considerable amount of memory and overhead of context switching \cite{seda}. Having thousands of connections to such a server cannot handle large requests. In order to overcome this problem SEDA (Staged event driven architecture) \cite{seda} were proposed. It limits the thread count using the concept of using thread pools, thus it reduces the context switching and memory usage. SEDA is a very successful mechanism to handle large concurrencies. Although the question had arisen “do SEDA perform well in every situation?”. \cite{Scalable_Threads_for_Internet_Services,events_are_bad,event_deriven_programming_for_robust_software} This debate has a long history. To answer this there were several studies conducted and proposed several fine-tuned architectures for refined situations.

\textit{Bharti et al.} \cite{fine_grained_SEDA} proposed a fine Grained SEDA Architecture for Service-Oriented Network Management Systems in order to improve the performance architecture for specific functions. They are fine-tuning \acrshort{seda} by breaking the stage in SEDA into sub-stages. Each sub-stage represents a SEDA implementation of critical functionality of the parent SEDA stage where each stage consists of a single thread pool. In those stages, performance has been improved by micro-managing the functionalities of each sub stage. Hence, the application, instead of having a single stage with competing threads from their functionalities, will now have fine-grained sub-stages. Each of the fine-grained sub-stages will now host threads performing a much smaller set of tasks. They showed that design has significant performance improvement of the application. 

\textit{Pariag et al.} \cite{comp_ac}  showed extensive tuning of web server architectures able to significantly improve the performance of specific types of workloads. Workload refers to what type of operation expects to be carried out in the web server, especially IO operations. They have considered 3 different server libraries that have different server architectures, the $\mu$server utilizes an event-driven architecture, Knot that uses the highly-efficient Capriccio thread library for implementing a thread per connection model, and WatPipe that uses a hybrid of events and threads to implement a pipeline based server that is similar to staged event-driven architecture \acrfull{seda} server like Haboob \cite{seda}. They have modified the Capriccio thread library to use Linux's zero-copy sendfile interface. Then they have introduced the \acrfull{SYMPED} architecture in which relatively minor modifications are made to a single process event-driven (SPED) \cite{flash_server} server (the $\mu$server) to allow it to continue processing requests in the presence of blocking IO due to disk accesses. They finally describe a C++ implementation of WatPipe, which although utilizing a pipeline-based architecture, excludes the dynamic controls over event queues and thread pools used in SEDA. 

They have compared all three architectures and the conclusion is slightly different from the previous studies. One important conclusion of them related to this research is they have observed that when using blocking sockets to send data to clients, the performance obtained with some architectures is quite good and in one case is noticeably better than when using non-blocking sockets. They have shown that a proper combination of the number of connections and kernel-level worker threads (or processes) is required to get the best performance results for each workload type.

In their conclusion they have stated  “this work could be done after or in conjunction with work that enables servers to automatically and dynamically tune them-selves to efficiently execute the offered workload”

More recent similar works \cite{comparing_high_performance_multi_core,uniproc_multiproc} can be found regarding improving the performance of web server architectures. Thus, we can conclude the performance of web server architecture is dependent on the type of workload or type of operations which web server executes. Tuning parameters such as thread pool size of web server also able to improve the performance.

Behren et al. \cite{events_are_bad} claiming that “Events Are A Bad Idea (for high-concurrency servers)”. They have shown a comparison of benchmark results of threaded and event-based architectures. In their conclusion stated, “Although event systems have been used to obtain good performance in high concurrency systems, we have shown that similar or even higher performance can be achieved with threads”. But there is a question left on whether it holds true only for the Java \acrshort{NIO} library or for the non-blocking approach in general.

\subsection{Ballerina language}

As the growing needs of the distributed computing and rapid development of web services, new programming
languages and paradigms were emerged. Ballerina \cite{ballerina} is such a successful language that handles
network programming in a developer-friendly way. Here I quoted a small paragraph from the ballerina
website.

“For decades, programming languages have treated networks simply as I/O sources. Ballerina
introduces fundamental, new abstractions of client objects, services, resource functions, and listeners
to bring networking into the language so that programmers can directly address the Fallacies \cite{fallacies_of_distributed_computing} of
Distributed Computing as part of their application logic. This facilitates resilient, secure, performant
network applications to be within every programmer’s reach.”

Apart from the above quotation, Ballerina is best suited for this research since Ballerina language has certain capabilities extracting performance-critical features especially IO calls. In Ballerina, they are called connector calls. In order to answer the second research question, this provides huge benefits as network calls are first-class citizens in Ballerina languages. Chapter \ref{chap:3} explains more about these features.

While other languages provide network operations as library functions Ballerina provides in a native way. If we try to extract such features from other languages we need to know exactly which function call does an IO operation including library functions. That is a very difficult task to extract such features just by exploring the source code in other languages like C, Java, etc.

Network services are first-order objects in ballerina language similar functions. \cite{ballerina_book} Such features
can be used to identify whether a given program contains network services or not. In other languages, we need to have prior knowledge about which libraries are used for such implementations and which part of the code implements the network services.


There are some tools already developed \cite{ballerina_plugin_vs_code,ballerina_plugin_intelij} using these features in order to represent the code as sequence diagrams. Figure \ref{diagram_view_ballerina_code} shows a generated sequence diagram for a code that includes a remote database call.

 \begin{figure}[htbp]
	\begin{center}
		\includegraphics[scale=0.7]{figures/diagram_view_db_call.png}
	\end{center}
	\caption{Sequence diagram representation of code}
	\label{diagram_view_ballerina_code}
\end{figure}
 
It is clear that we can use the network awareness properties of the Ballerina language to identify
performance-critical features directly from source code, unlike other languages. Examples of network
awareness features are creating RESTful clients and servers, making database queries, completing
asynchronous calls, producing and consuming streaming data message-passing etc \cite{ballerina_book}.

\subsection{Thread pool tuning}

Thread pools are a very important component when implementing server architectures. In SEDA every stage has its own thread pool. Creating and destroying a thread is resource intensive task for the OS \cite{thread_pool_analysis}. Instead of creating and destroying threads on demand, thread pool reuses the allocated number of threads. The size of the thread pool is almost fixed, although there are many studies to propose the number of threads in the pool dynamically using various approaches in order to gain the performance. Some approaches propose analytical models and schemes \cite{xu2004performance,thread_pool_analysis,math_aproach_thread_pool_tuning,syer2011identifying,linfeng2017design} and other approaches use run-time information \cite{lorenzon2016investigating,nieplocha2007evaluating,agrawal2006adaptive} to predict size of the pool. Most server implementation decides the number of thread in the pool heuristically and based on empirical knowledge \cite{thread_pool_analysis,math_aproach_thread_pool_tuning} to get balanced performance for every type of operations including IO since integrating analytical modeling is complex and calculation of run-time information can add an overhead to the server.

When discussing some heuristic and experience based approaches to decide the thread pool size, one rule is that the size of the thread pool should be two times the number of CPUs (processors). Ballerina scheduler is also following this rule. Another rule, used by Microsoft's IIS, initially allocates 10 threads in the thread pool per CPU, and allows the size of the thread pool to increase based on the number of client requests \cite{thread_pool_analysis}. The thread pool's maximum size is heuristically set to be two times the number of megabytes of RAM in the host machine. Those heuristics are based on memory and CPU capabilities and there is no theoretical justification.

\textit{Yibei Ling} et al. \cite{thread_pool_analysis} propose a mathematical model to predict optimal thread pool size by calculating the cost of spawning, destroying and maintaining threads in a thread pool. However, this approach has some flaws because of the assumptions they initially made. The first assumption is that each thread in the pool has the same execution priority and receives uniform treatment. This is not always true because OS has a limited number of processors and there are other OS-level processes also using those resources. Typical modern web servers are deployed with resource control and monitoring tools as well. Thus threads may not get equal access to CPUs. The next assumption is one web request is very much like another and that the memory and I/O resources required by different threads are minimal. This research's major focus is to optimize the server architectures for different types of programming features especially IO. 

D.L. Freire et al.\cite{math_aproach_thread_pool_tuning} also did a similar study to calculate the expected gain of web servers under different thread pool sizes using an analytical model. The optimal size of the pool depends on the cost of creating threads, the cost of maintenance of the pool and the workload of the system as a probability density function. The workload can be thought of as the rate of receiving the client request to the web server. Experiments were evaluated under four different probability density functions which are uniform, exponential, density of Pareto and gamma density. Then optimal thread pool size is decided for each density function. However, this study has a similar assumption to the above study. 
 
There are a number of parameters that can be tuned in a thread pool such as number of threads in the pool, maximum number of threads allowed in a pool, maximum time interval that a thread will be idle waiting for a new task \cite{math_aproach_thread_pool_tuning}. However, most of the studies are focused only to tune the size of thread pool size since that parameter affects most when evaluating the performance of web servers.

\begin{comment}

\subsection{Static Program analysis for performance}

Program analysis can be divided into static analysis and dynamic analysis. There are some previous studies which have been put effort on static analysis and dynamic analysis for performance modeling while relatively fewer studies have been done for static analysis for performance modeling.  

\end{comment}

\subsection{Importance of optimization} 

Optimization for web server is important for a business, so services able to deliver better performance using minimum resources. Today, a larger portion of businesses are dependent on web services. Growing needs demand more resources that cost for the business. Although the word "optimization" shares the same root as "optimal", it is rare for the process of optimization to produce a truly optimal system. It is hard to define an optimal system where the system is optimized under certain quality metrics, which may be in contrast with other possible metrics. Therefore, the optimized system will typically only be optimal in one set of applications, not for all. One might reduce the CPU usage, while one is optimizing memory usage. Often there is no "one size fits all" design that works well in all cases, thus there are trade-offs to optimize the attributes of the greatest interest.

When coming into web server optimization, it can be achieved in different ways. One way is to write clever code that uses better data structures and algorithms, place code in perfect places in order not to waste CPU for IO. Programming patterns like asynchronous programming patterns provide some shortcuts for developers in order to overcome blocking scenarios. Still, optimization is hard for every aspect because interrelation of trade-off between techniques \cite{wescott2013every,Adewumi+2018+115+122}.  Another way is to optimize web servers where business logic is executing. Then developers able to focus more on business logic rather than spending time on code optimization.



\section{Conclusion}

It is clear that the performance of a web server can be depended on the type of operation that is carried out by the web server. Moreover, IO features affect the performance of web servers and some web server architectures perform well for certain use cases. Also, careful tuning of web servers was able to achieve significant performance improvement. Furthermore, thread pool tuning is dominant where the domain of optimizing the performance of server architectures. It is understandable why analytical models are sometimes not practical in certain situations. Finally, no research could be found that program classification is based on how programming features affect the performance of web servers.   

\begin{comment}

	Although the word "optimization" shares the same root as "optimal", it is rare for the process of optimization to produce a truly optimal system. A system can generally be made optimal not in absolute terms, but only with respect to a given quality metric, which may be in contrast with other possible metrics. As a result, the optimized system will typically only be optimal in one application or for one audience. One might reduce the amount of time that a program takes to perform some task at the price of making it consume more memory. In an application where memory space is at a premium, one might deliberately choose a slower algorithm in order to use less memory. Often there is no "one size fits all" design which works well in all cases, so engineers make trade-offs to optimize the attributes of greatest interest. Additionally, the effort required to make a piece of software completely optimal – incapable of any further improvement – is almost always more than is reasonable for the benefits that would be accrued; so the process of optimization may be halted before a completely optimal solution has been reached. Fortunately, it is often the case that the greatest improvements come early in the process.
	
	Even for a given quality metric (such as execution speed), most methods of optimization only improve the result; they have no pretense of producing optimal output. Superoptimization is the process of finding truly optimal output.
	
	\subsection{The IO and Programming}

	
	In early days one major reason to use blocking code (synchronous programming) is that it is easy to write
	programs. Programmer always knows the order of code which is executing. When there is a blocking IO call ,in most cases, the wait is not really a problem because the program can not perform anything else until the IO is finished. However, in cases such as network programming with multiple
	clients, the program may wish to perform other activity as it continues to wait for more concurrent
	users.
	One solution for these situations is to use multiple threads so that one part of the program is not
	waiting for unrelated IO to complete. But having many threads add large overhead to memory and
	CPU ( thread creation and context switching is costly)
	Another alternative is to use asynchronous programming techniques with non-blocking system calls.
	An asynchronous call returns immediately, without waiting for the IO to complete. The completion of
	the IO is later communicated to the application either through the setting of some variable in the application or through the triggering of a signal or call-back routine that is executed outside the linear
	control flow of the application.
	New programming languages and patterns have been adopted for exponential growth of web service
	requirements and rapid development. But utilizing the program to use it’s full potential and use
	resources efficiently is still a problem. Developers or programmers need to put more effort into
	optimizing the programs.
	One such optimization way is to write clever codes to use the blocking or waiting time on CPU for
	other tasks. In order to do that, programmers need to know where the IO calls happen. But using many
	libraries and knowing where the IO calls happen is a very complex task. Even though it could be
	found such calls optimizing is still hard.
	
\end{comment}






